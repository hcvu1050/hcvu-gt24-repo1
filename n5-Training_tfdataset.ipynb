{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.losses import BinaryCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_train_df      = pd.read_csv ('data/preprocessed_data/G_train.csv')\n",
    "T_train_df      = pd.read_csv ('data/preprocessed_data/T_train.csv')\n",
    "G_T_train_df    = pd.read_csv ('data/preprocessed_data/balanced_G_T_train.csv')\n",
    "\n",
    "G_cv_df         = pd.read_csv ('data/preprocessed_data/G_cv.csv')\n",
    "T_cv_df         = pd.read_csv ('data/preprocessed_data/T_cv.csv')\n",
    "G_T_cv_df        = pd.read_csv ('data/preprocessed_data/G_T_cv.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 13\n",
    "frac = 0.1\n",
    "\n",
    "G_train_df      = G_train_df.sample(frac = frac, random_state=random_state)\n",
    "T_train_df      = T_train_df.sample(frac = frac, random_state=random_state)\n",
    "G_T_train_df    = G_T_train_df.sample(frac = frac, random_state=random_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12879, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_T_train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = G_T_train_df.drop(columns = ['group ID','technique ID' ]).values\n",
    "y_train.dtype\n",
    "\n",
    "# G_train = sampled_G_train_df.drop(columns = ids)\n",
    "G_train = G_train_df.drop(columns = 'group ID').values\n",
    "\n",
    "# T_train = sampled_T_train_df.drop(columns = ids)\n",
    "T_train = T_train_df.drop(columns = 'technique ID').values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "dataset = tf.data.Dataset.from_tensor_slices(({'input_Group': G_train_tf, 'input_Technique': T_train_tf}, y_train_tf))\n",
    "```\n",
    "\n",
    "1. `tf.data.Dataset`: This is a class in TensorFlow that represents a potentially large collection of elements. It's used to create pipelines for data processing and input to machine learning models.\n",
    "\n",
    "2. `from_tensor_slices`: This is a method of the `tf.data.Dataset` class. It is used to create a dataset by slicing the given tensors along their first dimension (usually representing the number of samples). This is a convenient way to create a dataset where each element corresponds to a pair of inputs and their corresponding targets.\n",
    "\n",
    "3. `({'input_Group': G_train_tf, 'input_Technique': T_train_tf}, y_train_tf)`: This is a tuple containing two elements. The first element is a dictionary that maps the input tensor names (`'input_Group'` and `'input_Technique'`) to their corresponding tensor data (`G_train_tf` and `T_train_tf`). The second element is the target tensor data `y_train_tf`. ‚ùóThe  names used in the from_tensor_slices dictionary should match the names of the input layers in your model. This ensures that the data from the dataset is correctly mapped to the corresponding input layers during training.\n",
    "\n",
    "\n",
    "So, the overall process of this line of code is to create a dataset where each element is a pair of dictionaries (`{'input_Group': ..., 'input_Technique': ...}`) representing the input tensors and their corresponding target tensor.\n",
    "\n",
    "When you iterate through this dataset, you'll get pairs like:\n",
    "```\n",
    "({'input_Group': G_train_sample, 'input_Technique': T_train_sample}, y_train_sample)\n",
    "```\n",
    "\n",
    "Here, `G_train_sample` and `T_train_sample` are individual samples from your `input_Group` and `input_Technique` tensors, and `y_train_sample` is the corresponding target value for that sample. This dataset structure is suitable for training machine learning models where you have multiple input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_train_tf = tf.convert_to_tensor(G_train)\n",
    "T_train_tf = tf.convert_to_tensor(T_train)\n",
    "y_train_tf = tf.convert_to_tensor(y_train)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(({'input_Group': G_train_tf, 'input_Technique': T_train_tf}, y_train_tf))\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.shuffle(buffer_size=len(G_train_tf))\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cv = G_T_cv_df.drop(columns = ['group ID','technique ID' ]).values\n",
    "y_cv.dtype\n",
    "\n",
    "# G_cv = sampled_G_cv_df.drop(columns = ids)\n",
    "G_cv = G_cv_df.drop(columns = 'group ID').values\n",
    "\n",
    "# T_cv = sampled_T_cv_df.drop(columns = ids)\n",
    "T_cv = T_cv_df.drop(columns = 'technique ID').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_cv_tf = tf.convert_to_tensor(G_cv)\n",
    "T_cv_tf = tf.convert_to_tensor(T_cv)\n",
    "y_cv_tf = tf.convert_to_tensor(y_cv)\n",
    "cv_dataset = tf.data.Dataset.from_tensor_slices(({'input_Group': G_cv_tf, 'input_Technique': T_cv_tf}, y_cv_tf))\n",
    "cv_dataset = cv_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input shapes config\n",
    "num_G_features = G_train.shape[1]  # remove Group ID during training\n",
    "num_T_features = T_train.shape[1]   # remove Movie ID during training\n",
    "\n",
    "# output\n",
    "num_outputs = 32\n",
    "\n",
    "tf.random.set_seed(random_state)\n",
    "\n",
    "# Group NN\n",
    "Group_NN = tf.keras.models.Sequential(\n",
    "    layers=[\n",
    "    tf.keras.layers.Dense (256, activation = 'relu'),\n",
    "    tf.keras.layers.Dense (128, activation = 'relu'),\n",
    "    tf.keras.layers.Dense (128, activation = 'relu'),\n",
    "    tf.keras.layers.Dense (num_outputs, activation  = 'linear'),\n",
    "    ], \n",
    "    name= \"Group_NN\")\n",
    "# input vector for user_NN\n",
    "input_Group = tf.keras.layers.Input(shape = (num_G_features), name = \"input_Group\")\n",
    "vg = Group_NN(input_Group)\n",
    "# vg = tf.linalg.l2_normalize(vg, axis=1)\n",
    "\n",
    "# Technique NN\n",
    "Technique_NN = tf.keras.models.Sequential(\n",
    "    layers = [\n",
    "    tf.keras.layers.Dense (256, activation = 'relu'),\n",
    "    tf.keras.layers.Dense (128, activation = 'relu'),\n",
    "    tf.keras.layers.Dense (128, activation = 'relu'),\n",
    "    tf.keras.layers.Dense (num_outputs, activation  = 'linear'),  \n",
    "    ],\n",
    "    name = \"Technique_NN\")\n",
    "# input vector for Technique_NN\n",
    "input_Technique = tf.keras.layers.Input (shape= (num_T_features), name = \"input_Technique\")\n",
    "vt = Technique_NN (input_Technique)\n",
    "# vt = tf.linalg.l2_normalize (vt, axis = 1)\n",
    "\n",
    "output = tf.keras.layers.Dot (axes=1)(inputs= [vg, vt])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(random_state)\n",
    "opt = keras.optimizers.Adam (learning_rate= 0.05)\n",
    "model = tf.keras.Model (inputs = [input_Group, input_Technique],\n",
    "                        outputs = output, name = 'recsysNN_model')\n",
    "model.compile (optimizer = opt, loss = BinaryCrossentropy (from_logits= True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',  # Monitor validation loss for early stopping\n",
    "    patience=32,           # Number of epochs with no improvement before stopping\n",
    "    restore_best_weights=True  # Restore model weights from the epoch with the best validation loss\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "403/403 [==============================] - 3s 5ms/step - loss: 1.1060 - val_loss: 0.6375\n",
      "Epoch 2/100\n",
      "403/403 [==============================] - 2s 4ms/step - loss: 0.7051 - val_loss: 0.6718\n",
      "Epoch 3/100\n",
      "403/403 [==============================] - 2s 4ms/step - loss: 0.7046 - val_loss: 0.6549\n",
      "Epoch 4/100\n",
      "403/403 [==============================] - 2s 4ms/step - loss: 0.6940 - val_loss: 0.7066\n",
      "Epoch 5/100\n",
      "403/403 [==============================] - 2s 4ms/step - loss: 0.6933 - val_loss: 0.6942\n",
      "Epoch 6/100\n",
      "403/403 [==============================] - 2s 5ms/step - loss: 0.6932 - val_loss: 0.6939\n",
      "Epoch 7/100\n",
      "403/403 [==============================] - 2s 4ms/step - loss: 0.6932 - val_loss: 0.6949\n",
      "Epoch 8/100\n",
      "403/403 [==============================] - 2s 4ms/step - loss: 0.6932 - val_loss: 0.6932\n",
      "Epoch 9/100\n",
      "403/403 [==============================] - 2s 4ms/step - loss: 0.6931 - val_loss: 0.6931\n",
      "Epoch 10/100\n",
      "403/403 [==============================] - 2s 4ms/step - loss: 0.6931 - val_loss: 0.6931\n",
      "Epoch 11/100\n",
      "403/403 [==============================] - 2s 4ms/step - loss: 0.6931 - val_loss: 0.6932\n",
      "Epoch 12/100\n",
      "403/403 [==============================] - 2s 4ms/step - loss: 0.6931 - val_loss: 0.6931\n",
      "Epoch 13/100\n",
      "403/403 [==============================] - 2s 4ms/step - loss: 0.6931 - val_loss: 0.6931\n",
      "Epoch 14/100\n",
      "403/403 [==============================] - 2s 4ms/step - loss: 0.6933 - val_loss: 0.7461\n",
      "Epoch 15/100\n",
      "403/403 [==============================] - 2s 4ms/step - loss: 0.6934 - val_loss: 0.7034\n",
      "Epoch 16/100\n",
      "403/403 [==============================] - 2s 4ms/step - loss: 0.6967 - val_loss: 0.7051\n",
      "Epoch 17/100\n",
      "403/403 [==============================] - 2s 4ms/step - loss: 0.6961 - val_loss: 0.6906\n",
      "Epoch 18/100\n",
      "403/403 [==============================] - 2s 4ms/step - loss: 0.6953 - val_loss: 0.6560\n",
      "Epoch 19/100\n",
      "403/403 [==============================] - 2s 4ms/step - loss: 0.6957 - val_loss: 0.6660\n",
      "Epoch 20/100\n",
      "403/403 [==============================] - 2s 4ms/step - loss: 0.6946 - val_loss: 0.6983\n",
      "Epoch 21/100\n",
      "403/403 [==============================] - 2s 5ms/step - loss: 0.6940 - val_loss: 0.7663\n",
      "Epoch 22/100\n",
      "403/403 [==============================] - 2s 4ms/step - loss: 0.6958 - val_loss: 0.7428\n",
      "Epoch 23/100\n",
      "403/403 [==============================] - 2s 5ms/step - loss: 0.6955 - val_loss: 0.7101\n",
      "Epoch 24/100\n",
      "403/403 [==============================] - 2s 4ms/step - loss: 0.6957 - val_loss: 0.6980\n",
      "Epoch 25/100\n",
      "403/403 [==============================] - 2s 4ms/step - loss: 0.6935 - val_loss: 0.6979\n",
      "Epoch 26/100\n",
      "403/403 [==============================] - 2s 4ms/step - loss: 0.6968 - val_loss: 0.6885\n",
      "Epoch 27/100\n",
      "403/403 [==============================] - 2s 4ms/step - loss: 0.6989 - val_loss: 0.7030\n",
      "Epoch 28/100\n",
      "403/403 [==============================] - 2s 4ms/step - loss: 0.6937 - val_loss: 0.7572\n",
      "Epoch 29/100\n",
      "403/403 [==============================] - 2s 4ms/step - loss: 0.6962 - val_loss: 0.6943\n",
      "Epoch 30/100\n",
      "403/403 [==============================] - 2s 4ms/step - loss: 0.6936 - val_loss: 0.7053\n",
      "Epoch 31/100\n",
      "403/403 [==============================] - 2s 4ms/step - loss: 0.6941 - val_loss: 0.6946\n",
      "Epoch 32/100\n",
      "403/403 [==============================] - 2s 4ms/step - loss: 0.6959 - val_loss: 0.7306\n",
      "Epoch 33/100\n",
      "403/403 [==============================] - 2s 4ms/step - loss: 0.6948 - val_loss: 0.6750\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "history = model.fit(train_dataset,\n",
    "                    validation_data = cv_dataset,\n",
    "                    epochs = epochs,\n",
    "                    callbacks = [early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "training_loss = history.history['loss']\n",
    "validation_loss = history.history['val_loss']\n",
    "\n",
    "training_loss_file = 'training_loss.csv'\n",
    "validation_loss_file = 'validation_loss.csv'\n",
    "\n",
    "with open(training_loss_file, mode='w', newline='') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow(training_loss)\n",
    "\n",
    "with open(validation_loss_file, mode='w', newline='') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow(validation_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "print(len(training_loss))\n",
    "print(len(validation_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.1059573888778687, 0.7051196098327637, 0.7046013474464417, 0.6940036416053772, 0.6932709813117981, 0.6931689977645874, 0.6931513547897339, 0.6931578516960144, 0.6931471228599548, 0.6931462287902832, 0.6931464076042175, 0.6931467056274414, 0.6931459903717041, 0.6933404803276062, 0.6933574676513672, 0.6967170238494873, 0.6961445212364197, 0.6953396201133728, 0.6956703066825867, 0.6945726871490479, 0.6939758658409119, 0.6958285570144653, 0.6955490112304688, 0.6956813335418701, 0.6934887170791626, 0.6967905163764954, 0.6989401578903198, 0.6936697363853455, 0.6961933970451355, 0.69358891248703, 0.694071888923645, 0.6959228515625, 0.6948370933532715]\n",
      "[0.6374624967575073, 0.6718109250068665, 0.6548517346382141, 0.7065610885620117, 0.6941655874252319, 0.693869411945343, 0.6948541402816772, 0.693224310874939, 0.6931468844413757, 0.6931461095809937, 0.6931502819061279, 0.6931461095809937, 0.6931461095809937, 0.746077299118042, 0.7034499645233154, 0.7051395773887634, 0.6906052827835083, 0.6559537649154663, 0.6659599542617798, 0.6983059048652649, 0.7663426399230957, 0.7427623867988586, 0.7100832462310791, 0.6979941725730896, 0.6979095339775085, 0.6884968280792236, 0.7030383348464966, 0.7571817636489868, 0.6942681670188904, 0.705311119556427, 0.6946442723274231, 0.7305793166160583, 0.6750392913818359]\n"
     ]
    }
   ],
   "source": [
    "print (training_loss)\n",
    "print (validation_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation loss\n",
    "# plt.plot(range(1, len(training_loss) + 1), training_loss, label='Training Loss')\n",
    "# plt.plot(range(1, len(validation_loss) + 1), validation_loss, label='Validation Loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(range(1, len(training_loss) + 1), training_loss, label='Training Loss')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfNote01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
