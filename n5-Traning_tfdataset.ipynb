{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.losses import BinaryCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_train_df      = pd.read_csv ('data/preprocessed_data/G_train.csv')\n",
    "T_train_df      = pd.read_csv ('data/preprocessed_data/T_train.csv')\n",
    "G_T_train_df    = pd.read_csv ('data/preprocessed_data/balanced_G_T_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 13\n",
    "frac = 0.2\n",
    "\n",
    "sampled_G_train_df      = G_train_df.sample(frac = frac, random_state=random_state)\n",
    "sampled_T_train_df      = T_train_df.sample(frac = frac, random_state=random_state)\n",
    "sampled_G_T_train_df    = G_T_train_df.sample(frac = frac, random_state=random_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25757, 3)\n",
      "(25757, 464)\n",
      "(25757, 55)\n"
     ]
    }
   ],
   "source": [
    "print (sampled_G_T_train_df.shape)\n",
    "print (sampled_G_train_df.shape)\n",
    "print (sampled_T_train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = sampled_G_T_train_df.drop(columns = ['group ID','technique ID' ]).values\n",
    "y_train.dtype\n",
    "\n",
    "# G_train = sampled_G_train_df.drop(columns = ids)\n",
    "G_train = sampled_G_train_df.drop(columns = 'group ID').values\n",
    "\n",
    "# T_train = sampled_T_train_df.drop(columns = ids)\n",
    "T_train = sampled_T_train_df.drop(columns = 'technique ID').values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input shapes config\n",
    "num_G_features = G_train.shape[1]  # remove Group ID during training\n",
    "num_T_features = T_train.shape[1]   # remove Movie ID during training\n",
    "\n",
    "# output\n",
    "num_outputs = 32\n",
    "\n",
    "tf.random.set_seed(random_state)\n",
    "\n",
    "# Group NN\n",
    "Group_NN = tf.keras.models.Sequential(\n",
    "    layers=[\n",
    "    tf.keras.layers.Dense (256, activation = 'relu'),\n",
    "    tf.keras.layers.Dense (128, activation = 'relu'),\n",
    "    tf.keras.layers.Dense (num_outputs, activation  = 'linear'),\n",
    "    ], \n",
    "    name= \"Group_NN\")\n",
    "# input vector for user_NN\n",
    "input_Group = tf.keras.layers.Input(shape = (num_G_features), name = \"input_Group\")\n",
    "vg = Group_NN(input_Group)\n",
    "# vg = tf.linalg.l2_normalize(vg, axis=1)\n",
    "\n",
    "# Technique NN\n",
    "Technique_NN = tf.keras.models.Sequential(\n",
    "    layers = [\n",
    "    tf.keras.layers.Dense (256, activation = 'relu'),\n",
    "    tf.keras.layers.Dense (128, activation = 'relu'),\n",
    "    tf.keras.layers.Dense (num_outputs, activation  = 'linear'),  \n",
    "    ],\n",
    "    name = \"Technique_NN\")\n",
    "# input vector for Technique_NN\n",
    "input_Technique = tf.keras.layers.Input (shape= (num_T_features), name = \"input_Technique\")\n",
    "vt = Technique_NN (input_Technique)\n",
    "# vt = tf.linalg.l2_normalize (vt, axis = 1)\n",
    "\n",
    "output = tf.keras.layers.Dot (axes=1)(inputs= [vg, vt])\n",
    "\n",
    "model = tf.keras.Model (inputs = [input_Group, input_Technique],\n",
    "                        outputs = output, name = 'recsysNN_model')\n",
    "\n",
    "\n",
    "\n",
    "tf.random.set_seed(random_state)\n",
    "opt = keras.optimizers.Adam (learning_rate= 0.05)\n",
    "model.compile (optimizer = opt, loss = BinaryCrossentropy (from_logits= True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "805/805 [==============================] - 4s 4ms/step - loss: 0.7532\n",
      "Epoch 2/30\n",
      "805/805 [==============================] - 3s 4ms/step - loss: 0.6379\n",
      "Epoch 3/30\n",
      "805/805 [==============================] - 3s 4ms/step - loss: 0.7589\n",
      "Epoch 4/30\n",
      "805/805 [==============================] - 3s 4ms/step - loss: 0.6811\n",
      "Epoch 5/30\n",
      "805/805 [==============================] - 3s 4ms/step - loss: 0.7200\n",
      "Epoch 6/30\n",
      "805/805 [==============================] - 3s 4ms/step - loss: 0.6893\n",
      "Epoch 7/30\n",
      "805/805 [==============================] - 3s 4ms/step - loss: 0.6849\n",
      "Epoch 8/30\n",
      "805/805 [==============================] - 3s 4ms/step - loss: 0.6829\n",
      "Epoch 9/30\n",
      "805/805 [==============================] - 3s 4ms/step - loss: 0.7037\n",
      "Epoch 10/30\n",
      "805/805 [==============================] - 3s 4ms/step - loss: 0.6977\n",
      "Epoch 11/30\n",
      "805/805 [==============================] - 3s 4ms/step - loss: 0.6878\n",
      "Epoch 12/30\n",
      "805/805 [==============================] - 3s 4ms/step - loss: 0.6877\n",
      "Epoch 13/30\n",
      "805/805 [==============================] - 3s 4ms/step - loss: 0.6891\n",
      "Epoch 14/30\n",
      "805/805 [==============================] - 3s 4ms/step - loss: 0.6880\n",
      "Epoch 15/30\n",
      "805/805 [==============================] - 3s 4ms/step - loss: 0.6943\n",
      "Epoch 16/30\n",
      "805/805 [==============================] - 3s 4ms/step - loss: 0.6893\n",
      "Epoch 17/30\n",
      "805/805 [==============================] - 3s 4ms/step - loss: 0.6946\n",
      "Epoch 18/30\n",
      "805/805 [==============================] - 3s 4ms/step - loss: 0.6907\n",
      "Epoch 19/30\n",
      "805/805 [==============================] - 3s 4ms/step - loss: 0.6926\n",
      "Epoch 20/30\n",
      "805/805 [==============================] - 3s 4ms/step - loss: 0.7027\n",
      "Epoch 21/30\n",
      "805/805 [==============================] - 3s 4ms/step - loss: 0.6961\n",
      "Epoch 22/30\n",
      "805/805 [==============================] - 3s 4ms/step - loss: 0.6942\n",
      "Epoch 23/30\n",
      "805/805 [==============================] - 3s 4ms/step - loss: 0.6934\n",
      "Epoch 24/30\n",
      "805/805 [==============================] - 3s 4ms/step - loss: 0.6947\n",
      "Epoch 25/30\n",
      "805/805 [==============================] - 3s 4ms/step - loss: 0.6936\n",
      "Epoch 26/30\n",
      "805/805 [==============================] - 3s 4ms/step - loss: 0.6950\n",
      "Epoch 27/30\n",
      "805/805 [==============================] - 3s 4ms/step - loss: 0.6955\n",
      "Epoch 28/30\n",
      "805/805 [==============================] - 3s 4ms/step - loss: 0.6942\n",
      "Epoch 29/30\n",
      "805/805 [==============================] - 3s 4ms/step - loss: 0.6946\n",
      "Epoch 30/30\n",
      "805/805 [==============================] - 3s 4ms/step - loss: 0.6945\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20b925b7130>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_train_dataset = tf.data.Dataset.from_tensor_slices(G_train)\n",
    "T_train_dataset = tf.data.Dataset.from_tensor_slices(T_train)\n",
    "y_train_dataset = tf.data.Dataset.from_tensor_slices(y_train)\n",
    "\n",
    "model.fit (\n",
    "    x = [G_train, T_train],\n",
    "    y =  y_train,\n",
    "    batch_size = 32,\n",
    "    epochs = 30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming you have your input datasets and target data as NumPy arrays\n",
    "# input_data_group = G_train     # Replace [...] with your actual input data for 'input_Group'\n",
    "# input_data_technique = T_train # Replace [...] with your actual input data for 'input_Technique'\n",
    "# target_output = y_train          # Replace [...] with your actual target output data\n",
    "\n",
    "# # Create separate TensorFlow datasets for 'input_Group', 'input_Technique', and target output\n",
    "# input_dataset_group = tf.data.Dataset.from_tensor_slices(input_data_group)\n",
    "# input_dataset_technique = tf.data.Dataset.from_tensor_slices(input_data_technique)\n",
    "# target_output_dataset = tf.data.Dataset.from_tensor_slices(target_output)\n",
    "\n",
    "# # Combine the input datasets and target output dataset into a single dataset using zip\n",
    "# # combined_dataset = tf.data.Dataset.zip((input_dataset_group, input_dataset_technique, target_output_dataset))\n",
    "\n",
    "# # Optionally, you can shuffle, batch, and prefetch the combined dataset\n",
    "# batch_size = 32\n",
    "# # combined_dataset = combined_dataset.shuffle(buffer_size=len(input_data_group))\n",
    "# # combined_dataset = combined_dataset.batch(batch_size)\n",
    "# # combined_dataset = combined_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# # Define your model architecture\n",
    "# Group_NN = tf.keras.models.Sequential(\n",
    "#     layers=[\n",
    "#         tf.keras.layers.Dense(256, activation='relu'),\n",
    "#         tf.keras.layers.Dense(128, activation='relu'),\n",
    "#         tf.keras.layers.Dense(num_outputs, activation='linear'),\n",
    "#     ],\n",
    "#     name=\"Group_NN\"\n",
    "# )\n",
    "\n",
    "# Technique_NN = tf.keras.models.Sequential(\n",
    "#     layers=[\n",
    "#         tf.keras.layers.Dense(256, activation='relu'),\n",
    "#         tf.keras.layers.Dense(128, activation='relu'),\n",
    "#         tf.keras.layers.Dense(num_outputs, activation='linear'),\n",
    "#     ],\n",
    "#     name=\"Technique_NN\"\n",
    "# )\n",
    "\n",
    "# input_layer_group = tf.keras.layers.Input(shape=(num_G_features), name=\"input_Group\")\n",
    "# input_layer_technique = tf.keras.layers.Input(shape=(num_T_features), name=\"input_Technique\")\n",
    "\n",
    "# vg = Group_NN(input_layer_group)\n",
    "# vt = Technique_NN(input_layer_technique)\n",
    "\n",
    "# output = tf.keras.layers.Dot(axes=1)(inputs=[vg, vt])\n",
    "\n",
    "# model = tf.keras.Model(inputs=[input_layer_group, input_layer_technique], outputs=output, name='recsysNN_model')\n",
    "\n",
    "# # Compile the model with an appropriate optimizer, loss, and metrics\n",
    "# model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "# # Train the model using the combined dataset\n",
    "# num_epochs = 10\n",
    "# model.fit(\n",
    "#     x = [input_data_group, input_data_technique],\n",
    "#     y = target_output,\n",
    "#     epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfNote01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
