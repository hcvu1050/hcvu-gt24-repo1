{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.losses import BinaryCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_train_df      = pd.read_csv ('data/preprocessed_data/G_train.csv')\n",
    "T_train_df      = pd.read_csv ('data/preprocessed_data/T_train.csv')\n",
    "G_T_train_df    = pd.read_csv ('data/preprocessed_data/balanced_G_T_train.csv')\n",
    "\n",
    "G_cv_df         = pd.read_csv ('data/preprocessed_data/G_cv.csv')\n",
    "T_cv_df         = pd.read_csv ('data/preprocessed_data/T_cv.csv')\n",
    "G_T_cv_df        = pd.read_csv ('data/preprocessed_data/G_T_cv.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 13\n",
    "frac = 0.15\n",
    "\n",
    "G_train_df      = G_train_df.sample(frac = frac, random_state=random_state)\n",
    "T_train_df      = T_train_df.sample(frac = frac, random_state=random_state)\n",
    "G_T_train_df    = G_T_train_df.sample(frac = frac, random_state=random_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19318, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_T_train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = G_T_train_df.drop(columns = ['group ID','technique ID' ]).values\n",
    "y_train.dtype\n",
    "\n",
    "# G_train = sampled_G_train_df.drop(columns = ids)\n",
    "G_train = G_train_df.drop(columns = 'group ID').values\n",
    "\n",
    "# T_train = sampled_T_train_df.drop(columns = ids)\n",
    "T_train = T_train_df.drop(columns = 'technique ID').values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "dataset = tf.data.Dataset.from_tensor_slices(({'input_Group': G_train_tf, 'input_Technique': T_train_tf}, y_train_tf))\n",
    "```\n",
    "\n",
    "1. `tf.data.Dataset`: This is a class in TensorFlow that represents a potentially large collection of elements. It's used to create pipelines for data processing and input to machine learning models.\n",
    "\n",
    "2. `from_tensor_slices`: This is a method of the `tf.data.Dataset` class. It is used to create a dataset by slicing the given tensors along their first dimension (usually representing the number of samples). This is a convenient way to create a dataset where each element corresponds to a pair of inputs and their corresponding targets.\n",
    "\n",
    "3. `({'input_Group': G_train_tf, 'input_Technique': T_train_tf}, y_train_tf)`: This is a tuple containing two elements. The first element is a dictionary that maps the input tensor names (`'input_Group'` and `'input_Technique'`) to their corresponding tensor data (`G_train_tf` and `T_train_tf`). The second element is the target tensor data `y_train_tf`. ‚ùóThe  names used in the from_tensor_slices dictionary should match the names of the input layers in your model. This ensures that the data from the dataset is correctly mapped to the corresponding input layers during training.\n",
    "\n",
    "\n",
    "So, the overall process of this line of code is to create a dataset where each element is a pair of dictionaries (`{'input_Group': ..., 'input_Technique': ...}`) representing the input tensors and their corresponding target tensor.\n",
    "\n",
    "When you iterate through this dataset, you'll get pairs like:\n",
    "```\n",
    "({'input_Group': G_train_sample, 'input_Technique': T_train_sample}, y_train_sample)\n",
    "```\n",
    "\n",
    "Here, `G_train_sample` and `T_train_sample` are individual samples from your `input_Group` and `input_Technique` tensors, and `y_train_sample` is the corresponding target value for that sample. This dataset structure is suitable for training machine learning models where you have multiple input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_train_tf = tf.convert_to_tensor(G_train)\n",
    "T_train_tf = tf.convert_to_tensor(T_train)\n",
    "y_train_tf = tf.convert_to_tensor(y_train)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(({'input_Group': G_train_tf, 'input_Technique': T_train_tf}, y_train_tf))\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.shuffle(buffer_size=len(G_train_tf))\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cv = G_T_cv_df.drop(columns = ['group ID','technique ID' ]).values\n",
    "y_cv.dtype\n",
    "\n",
    "# G_cv = sampled_G_cv_df.drop(columns = ids)\n",
    "G_cv = G_cv_df.drop(columns = 'group ID').values\n",
    "\n",
    "# T_cv = sampled_T_cv_df.drop(columns = ids)\n",
    "T_cv = T_cv_df.drop(columns = 'technique ID').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_cv_tf = tf.convert_to_tensor(G_cv)\n",
    "T_cv_tf = tf.convert_to_tensor(T_cv)\n",
    "y_cv_tf = tf.convert_to_tensor(y_cv)\n",
    "cv_dataset = tf.data.Dataset.from_tensor_slices(({'input_Group': G_cv_tf, 'input_Technique': T_cv_tf}, y_cv_tf))\n",
    "cv_dataset = cv_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input shapes config\n",
    "num_G_features = G_train.shape[1]  # remove Group ID during training\n",
    "num_T_features = T_train.shape[1]   # remove Movie ID during training\n",
    "\n",
    "# output\n",
    "num_outputs = 32\n",
    "\n",
    "tf.random.set_seed(random_state)\n",
    "\n",
    "# Group NN\n",
    "Group_NN = tf.keras.models.Sequential(\n",
    "    layers=[\n",
    "    tf.keras.layers.Dense (256, activation = 'relu'),\n",
    "    tf.keras.layers.Dense (128, activation = 'relu'),\n",
    "    tf.keras.layers.Dense (num_outputs, activation  = 'linear'),\n",
    "    ], \n",
    "    name= \"Group_NN\")\n",
    "# input vector for user_NN\n",
    "input_Group = tf.keras.layers.Input(shape = (num_G_features), name = \"input_Group\")\n",
    "vg = Group_NN(input_Group)\n",
    "# vg = tf.linalg.l2_normalize(vg, axis=1)\n",
    "\n",
    "# Technique NN\n",
    "Technique_NN = tf.keras.models.Sequential(\n",
    "    layers = [\n",
    "    tf.keras.layers.Dense (256, activation = 'relu'),\n",
    "    tf.keras.layers.Dense (128, activation = 'relu'),\n",
    "    tf.keras.layers.Dense (num_outputs, activation  = 'linear'),  \n",
    "    ],\n",
    "    name = \"Technique_NN\")\n",
    "# input vector for Technique_NN\n",
    "input_Technique = tf.keras.layers.Input (shape= (num_T_features), name = \"input_Technique\")\n",
    "vt = Technique_NN (input_Technique)\n",
    "# vt = tf.linalg.l2_normalize (vt, axis = 1)\n",
    "\n",
    "output = tf.keras.layers.Dot (axes=1)(inputs= [vg, vt])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(random_state)\n",
    "opt = keras.optimizers.Adam (learning_rate= 0.05)\n",
    "model = tf.keras.Model (inputs = [input_Group, input_Technique],\n",
    "                        outputs = output, name = 'recsysNN_model')\n",
    "model.compile (optimizer = opt, loss = BinaryCrossentropy (from_logits= True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',  # Monitor validation loss for early stopping\n",
    "    patience=5,           # Number of epochs with no improvement before stopping\n",
    "    restore_best_weights=True  # Restore model weights from the epoch with the best validation loss\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604/604 [==============================] - 5s 6ms/step - loss: 0.8916 - val_loss: 0.5091\n",
      "Epoch 2/30\n",
      "604/604 [==============================] - 3s 5ms/step - loss: 0.6242 - val_loss: 0.7446\n",
      "Epoch 3/30\n",
      "604/604 [==============================] - 4s 6ms/step - loss: 0.7102 - val_loss: 0.7789\n",
      "Epoch 4/30\n",
      "604/604 [==============================] - 4s 6ms/step - loss: 0.6779 - val_loss: 0.7535\n",
      "Epoch 5/30\n",
      "604/604 [==============================] - 3s 5ms/step - loss: 0.6738 - val_loss: 0.6759\n",
      "Epoch 6/30\n",
      "604/604 [==============================] - 4s 6ms/step - loss: 0.6735 - val_loss: 0.6569\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "history = model.fit(train_dataset,\n",
    "                    validation_data = cv_dataset,\n",
    "                    epochs = epochs,\n",
    "                    callbacks = [early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Access training and validation loss history\n",
    "training_loss = history.history['loss']\n",
    "validation_loss = history.history['val_loss']\n",
    "\n",
    "# Plot the training and validation loss\n",
    "# plt.plot(range(1, len(training_loss) + 1), training_loss, label='Training Loss')\n",
    "# plt.plot(range(1, len(validation_loss) + 1), validation_loss, label='Validation Loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print(len(training_loss))\n",
    "print(len(validation_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8915860652923584, 0.6242129802703857, 0.7102217674255371, 0.6779444217681885, 0.6737833619117737, 0.6735294461250305]\n",
      "[0.5090712904930115, 0.7446485757827759, 0.7788713574409485, 0.7535215020179749, 0.6758723258972168, 0.6569448113441467]\n"
     ]
    }
   ],
   "source": [
    "print (training_loss)\n",
    "print (validation_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation loss\n",
    "# plt.plot(range(1, len(training_loss) + 1), training_loss, label='Training Loss')\n",
    "# plt.plot(range(1, len(validation_loss) + 1), validation_loss, label='Validation Loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(range(1, len(training_loss) + 1), training_loss, label='Training Loss')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfNote01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
